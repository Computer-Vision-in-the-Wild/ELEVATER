<!DOCTYPE html>
<html lang="en">
    <head>
        <script src="https://kit.fontawesome.com/8bc47af9ac.js" crossorigin="anonymous"></script>
        <meta charset="utf-8">

        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>ELEVATER</title>
        <meta name="description" content="ELEVATER is a evaluation benchmark and toolkit built on 20 image classification datasets and 35 object detection datasets for Vision-Language Evaluation">
        <meta name="keywords" content="Vision-and-Language Benchmark, Image Classification, Object Detection, ToolKit">
        <meta name="author" content="ELEVATER">

        <link rel="icon" href="image/favicon.ico">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/fonts.css">
        <link rel="stylesheet" href="css/custom.css">
    </head>

    <body>
        <nav class="navbar navbar-expand-lg navbar-dark navbar-custom fixed-top">
            <div class="container">
                <a class="navbar-brand" href="index.html"><b>ELEVATER</b></a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#content" aria-controls="navbarsExample07" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
    
                <div class="collapse navbar-collapse" id="content">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item active">
                            <a class="nav-link" href="index.html"><b>Home</b></a>
                        </li>
                        <!-- <li class="nav-item">
                            <a class="nav-link" href="https://github.com/Computer-Vision-in-the-Wild/Toolkit" target="_blank">Code</a>
                        </li> -->
                        <li class="nav-item">
                            <a class="nav-link" href="download.html">Download</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="results.html">Results</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank">Workshop</a>
                        </li>
                        <!-- <li class="nav-item">
                            <a class="nav-link" href="submission.html">Submission</a>
                        </li> -->
                        <li class="nav-item">
                            <a class="nav-link" href="https://arxiv.org/abs/2204.08790" target="_blank">Paper</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="people.html">People</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>

        <div class="jumbotron">
            <div class="container">
            <h3 class="display-5 sharepoint-dark-blue">Hello ELEVATER! A Platform for Computer Vision in the Wild</h3>
            <p class="lead sharepoint-dark-blue"><b>E</b>valuation of <b>L</b>anguage-augm<b>e</b>nted <b>V</b>isual T<b>a</b>sk-level <b>T</b>ransf<b>er</b>.</p>
            </div>
        </div>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Why ELEVATER?</h4>
                    <hr>
                </div>
            </div>

            <div class="row">
                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <div class="card">
                        <div class="card-body">
                            <center>
                                <span class="fa-4x ms-blue">
                                    <i class="fa-solid fa-image"></i>
                                </span>
                            </center>
                            <br/>
                            <center><h5 class="card-title">Various Datasets over Representative Tasks</h5>
                            <p class="card-text"><font size="3rem"><b>20</b> image classification datasets / <b>35</b> object detection datasets.</font></p></center>
                        </div>
                    </div>
                    <br/>
                </div>

                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <div class="card">
                        <div class="card-body">
                            <center>
                                <span class="fa-4x ms-blue">
                                    <i class="fa-solid fa-toolbox"></i>
                                </span>
                            </center>
                            <br/>
                            <center><h5 class="card-title">Toolkit</h5>
                            <p class="card-text"><font size="3rem">Automatic hyper-parameter tuning; Strong language-augmented efficient adaptation methods</font></p></center>
                            <br/>
                        </div>
                    </div>
                    <br/>
                </div>

                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <div class="card">
                        <div class="card-body">
                            <center>
                                <span class="fa-4x ms-blue">
                                    <i class="fa-solid fa-brain"></i>
                                </span>
                            </center>
                            <br/>
                            <center><h5 class="card-title">Diverse Knowledge Source</h5>
                            <p class="card-text"><font size="3rem">Each dataset concept is augmented with diverse knowledge source include: WordNet, Wiktionary, and GPT3.</font></p></center>
                        </div>
                    </div>
                    <br/>
                </div>

                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <div class="card">
                        <div class="card-body">
                            <center>
                                <span class="fa-4x ms-blue">
                                    <i class="fa-solid fa-trophy"></i>
                                </span>
                            </center>
                            <br/>
                            <center><h5 class="card-title">Leaderboard!</h5>
                            <p class="card-text"><font size="3rem">To track the research advances in language-image models.</font></p></center>
                        </div>
                    </div>
                    <br/>
                </div>
            </div>
        </div>

        <br/>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>What is ELEVATER?</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">                
                    <p>The ELEVATER benchmark is a collection of resources for training, evaluating, and analyzing language-image models on image classification and object detection. ELEVATER consists of: </p>
                    <ul>
                        <li><b>Benchmark</b>: A benchmark suite that consists of 20 image classification datasets and 35 object detection datasets, augmented with external knowledge</li>
                        <li><b>Toolkit</b>: An automatic hyper-parameter tuning toolkit; Strong language-augmented efficient model adaptation methods.</li>
                        <li><b>Baseline</b>: Pre-trained languange-free and languange-augmented visual models.</li>
                        <li><b>Knowledge</b>: A platform to study the benefit of external knowledge for vision problems.</li>
                        <li><b>Evaluation Metrics</b>: Sample-efficiency (zero-, few-, and full-shot) and Parameter-efficiency.</li>
                        <li><b>Leaderboard</b>: A public leaderboard to track performance on the benchmark</li>
                    </ul>
                        
                    <p>The ultimate goal of ELEVATER is to drive research in the development of language-image models to tackle core computer vision problems in the wild.</p>
                    <p><a href="https://cvinw.blob.core.windows.net/benchmark/benchmark_elevater.pdf" target="_blank">[Quick introduction with slides]</a></p>
                </div>
            </div>
        </div>
            
        <br/>


        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>News</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">                


                    <ul>
                        <li><span style="font-size: 14px;" class="date badge badge-secondary">[ Sep 16, 2022 ]</span> Paper accepted in 
                            <a href="https://nips.cc/" target="_blank">
                        NeurIPS 2022
                            </a> Datasets and Benchmarks Track.</li>
                    </ul>

                    <ul>
                        <li>
                                <tr>

                                  <th scope="row"><span style="font-size: 14px;" class="date badge badge-secondary">[ Sep 1, 2022 ]</span>  </th>
                                  <td>  </td>
                                  <td>
                                    
                                      Call for Papers &amp; Participation: ECCV Workshop and Challenge on <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild (CVinW)</a>. 

                                        <table>
                                          <tr>
                                            <td style="width:220px">  <center>
                                        <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer"><img src="https://computer-vision-in-the-wild.github.io/eccv-2022/static/eccv2022/img/ECCV-logo3.png" alt="CVinW" width="100" height="52">   [Workshop]</a>  </center> </td>
                                            <td style="width:220px"> <center>
                                        <a href="https://eval.ai/web/challenges/challenge-page/1832/overview" target="_blank" rel="noopener noreferrer"><img src="https://evalai.s3.amazonaws.com/media/logos/4e939412-a9c0-46bd-9797-5ba0bd0a9095.jpg" alt="ICinW" width="100" height="52"> [IC Challenge]</a>   </center> </td>
                                            <td style="width:220px">  <center>
                                        <a href="https://eval.ai/web/challenges/challenge-page/1839/overview" target="_blank" rel="noopener noreferrer"><img src="https://evalai.s3.amazonaws.com/media/logos/e3727105-2b29-4c9b-98a6-3d1191884eb5.jpg" alt="ODinW" width="100" height="52"> [OD Challenge]</a>  </center> </td>
                                          </tr>
                                        </table>
                                  </td>
                                </tr>

                        </li>
                        
                    </ul>


                    <ul>

                        <li><span style="font-size: 14px;" class="date badge badge-secondary">[ Summer, 2022]</span> 
                             Interested in learning what is ``Computer Vision in the Wild''?
                             <ul>

                                <li>
                                    <span style="font-size: 14px;" class="date badge badge-secondary">Talks</span> 
                                    Please check out an overview of our team effort "A Vision-and-Language Approach to Computer Vision in the Wild: Modeling and Benchmark". Talks at 
                                <a href="https://machinelearning.apple.com/" target="_blank">Apple AI/ML</a>,
                                <a href="https://www.nist.gov/programs-projects/ai-measurement-and-evaluation/ai-metrology-colloquia-series" target="_blank">NIST</a>,
                                <a href="https://www.xiaoice.com/" target="_blank">Xiaoice</a>,
                                <a href="https://theaitalks.org/" target="_blank">The AI Talks</a>.
                                <a href="https://www.youtube.com/watch?v=_py7_JrsJ_8" target="_blank">
                            [YouTube]
                                </a>
                                </li>
                                <li><span style="font-size: 14px;" class="date badge badge-secondary">Demos</span> 
                                    Vision systems that are equipped with the mechanism to recognize any concept from any given images. Check out the demos on
                                    image classification with <a href="https://huggingface.co/spaces/CVPR/unicl-zero-shot-img-recog" target="_blank">UniCL</a>,
                                    object detection with 
                                    <a href="https://huggingface.co/spaces/CVPR/regionclip-demo" target="_blank">RegionCLIP</a> and
                                    <a href="https://github.com/microsoft/GLIP" target="_blank">GLIP</a>.
                                </li>
                                <li><span style="font-size: 14px;" class="date badge badge-secondary">Challenge</span> 
                                    Have a better idea? Join the <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank">community</a>.
                                </li>                                

                            </ul>

                        </li>


                        </ul>
                        <ul>

                        <li><span style="font-size: 14px;" class="date badge badge-secondary">[ June 19, 2022 ]</span>
                            <a href="https://vlp-tutorial.github.io/" target="_blank">
                        CVPR Tutorial
                            </a> on knowledge and benchmark on CVinW. 

                                
                            <a href="https://datarelease.blob.core.windows.net/tutorial/VLP-Tutorial_2022/vlp_for_v_part3.pdf" target="_blank">
                        [Slides]
                            </a> 
                            [<a href="https://www.youtube.com/watch?v=F519jcAppFA" target="_blank">YouTube</a>, 
                            <a href="https://www.bilibili.com/video/BV1FG411x7uB/" target="_blank">Bilibili</a>]                        
                        </li>

                        <li><span style="font-size: 14px;" class="date badge badge-secondary">[ Apr 19, 2022 ]</span> The first version is on 
                            <a href="https://arxiv.org/abs/2204.08790" target="_blank">arXiv</a>.
                        </li>
                    </ul>
   
                </div>
            </div>
        </div>
            
        <br/>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>A more diverse set of CV tasks</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <div class="card">
                        <div class="card-body">
                            <center>
                                <img src="image/comparison/benchmark_comparison_ic.png" width="450">
                            </center>
                            <!-- <br/>
                            <center>
                                <h5 class="card-title">IC</h5>
                            </center> -->
                        </div>
                    </div>
                    <br/>
                </div>
                <div class="col-lg-6 col-md-6 col-sm-12 col-xs-12 thumb">
                    <div class="card">
                        <div class="card-body">
                            <center>
                                <img src="image/comparison/benchmark_comparison_od.png" width="450">
                            </center>
                            <!-- <br/>
                            <center>
                                <h5 class="card-title">OD</h5>
                            </center> -->
                        </div>
                    </div>
                    <br/>
                </div>
            </div>
        </div>
            
        <br/>
        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Paper</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <a href="https://arxiv.org/abs/2204.08790" target="_blank">
                        <img src="image/paper.jpg" class="rounded img-fluid mx-auto d-block" border="5"/>
                    </a>
                </div>

                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                        <h6>Please cite our paper as below if you use the ELEVATER benchmark or our toolkit.</h6>
<pre><code>
@article{li2022elevater,
    title={ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models},
    author={Li, Chunyuan and Liu, Haotian and Li, Liunian Harold and Zhang, Pengchuan and Aneja, Jyoti and Yang, Jianwei and Jin, Ping and Hu, Houdong and Liu, Zicheng and Lee, Yong Jae and Gao, Jianfeng},
    journal={Neural Information Processing Systems},
    year={2022}
}
</code></pre>
                        <!-- TODO -->
                        <!-- <h6>We sincerely thank all dataset contributors to ELEVATER benchmark, please cite the following datasets if you use the ELEVATER benchmark.</h6> -->
                </div>
            </div>

        </div>

        <br/>

        <div class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Contact</h4>
                    <hr>
                </div>
            </div>

            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <a href="https://github.com/Computer-Vision-in-the-Wild/ELEVATER/issues" target="_blank">
                        <center><span class="fa-4x ms-blue">
                            <i class="fa-brands fa-github"></i>
                        </span></center>
                    </a>
                    <br/>
                    <center>
                    <h6>Have any questions or suggestions? Feel free to reach us by <a href="https://github.com/Computer-Vision-in-the-Wild/ELEVATER/issues" target="_blank">opening a GitHub issue</a>!</h6></center>
                </div>
            </div>

        </div>
        <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
        <script>window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')</script>
        <script src="js/popper.min.js"></script>
        <script src="js/bootstrap.min.js"></script>
        <script src="js/sorttable.js"></script>
        <script src="js/hide_display_citation_div.js"></script>
    </body>
</html>
